\section{Erzeugung von Sequenzen («Inferenz»)}
\label{sec:model-generation}

Das Erzeugen von neuen Sequenzen funktioniert ähnlich zum Training.
Allerdings entfällt der Optimierungsschritt, der das Modell trainiert.
Stattdessen wird anhand des ausgegebenen Wahrscheinlichkeitsvektors das vom Modell prognostizierte nächste Zeichen
ausgewählt und der Sequenz angehängt.
Die erweiterte Sequenz wird erneut in das Modell eingegegben und der Kreis schliesst sich.
Dieser Prozess wird so lange wiederholt, bis das Modell eine Stoppmarke («end») ausgibt oder die Anzahl erzeugter Zeichen eine bestimmte Sicherheitsmarke übersteigt (siehe Listing \ref{lst:generating-sequences}).

\begin{lstlisting}[language=Python, caption=Erzeugung von Sequenzen, label=lst:generating-sequences]
...

while last_char != '<end>' and i < 500:

    ...

    # Have model predict probabilities for next char
    probs = model.predict(x=np.array(padded_sequence)...)

    # Randomly choose the next char with p as bias
    last_char = np.random.choice(words, p=probs)

    # Append next char to sequence
    sequence += [last_char]

    i += 1

...
\end{lstlisting}

Der Aufruf \textit{model.predict(...)} gibt einen Wahrscheinlichkeitsvektor zurück, in dem jede Komponente der Wahrscheinlichkeit eines Zeichens im Vokabular entspricht, als nächstes Zeichen auf die bisherige Sequenz zu folgen.
So wie in \autocite{dabbura} soll der nächste Buchstabe zufällig ausgewählt werden.
Dabei wählt die NumPy-Funktion \textit{random.choice()} das nächste Zeichen gemäss der Wahrscheinlichkeitsverteilung $ p $ aus.
Diese Methode bewirkt, dass eine ins Modell eingegebene Initialsequenz jedes Mal andere Folgesequenzen erzeugt.
