\section{Erzeugung von Sequenzen}
\label{sec:model-generation}

Um mit dem trainierten Modell Sequenzen zu erzeugen, werden zufällige Zeichen als Initialsequenz erzeugt, die
wiederum ins Modell eingespeist werden.

Die ersten paar generierten Sequenzen sind in Tabelle \ref{tab:first-sequences} aufgeführt.
Interessanterweise zeigen sich bereits einige Muster, die entfernt an Namen von Gerichten erinnern.
So zum Beispiel «Artonast wit Creme» (4) oder «iAe, D' Arerel \& Sauce Potator».
Um das Modell weiter auszubauen und die Auswirkungen der Veränderungen konsistent beobachten zu können,
werden einige der zufälligen Initialsequenzen sowie ein paar manuell definierte Sequenzen als Ausgangslage
festgelegt (siehe Tabelle \ref{tab:fixed-initial-sequences}).

Rätselhaft ist vorerst, dass sich die erzeugten Sequenzen auch nach längeren Trainingszeiten (+4h) nicht merklich verbessern.
Nach längerer Analyse des Codes für das Training und Vergleichen mit Beispielmodellen\footnote{z.B. https:\/\/pytorch.org\/tutorials\/intermediate\/char\_rnn\_generation\_tutorial.html}
stellt sich heraus, dass ein Fehler in der Logik dafür sorgt, dass für jeden trainierten Namen immer nur der letzte Buchstabe trainiert wird (siehe Abb. \ref{fig:logic-error}).

Nachdem der Fehler behoben ist, stellen sich bereits nach dem Training eines verhältnismässig niederkomplexen Modells bedeutend bessere Resultate ein (siehe Tabelle \ref{tab:first-sequences-after-fix}).
Die Resultate weisen besser lesbare Abfolgen von Konsonanten und Vokalen auf.

Ausgehend von den Anfangs präsentierten Initialsequenzen sollen nun systematisch verschiedene Modell-Konfigurationen trainiert und
ausgewertet werden.
