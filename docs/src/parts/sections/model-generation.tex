\section{Erzeugung von Sequenzen («Inferenz»)}
\label{sec:model-generation}

Das Erzeugen von neuen Sequenzen funktioniert ähnlich zum Training.
Allerdings entfällt der Optimierungsschritt, der das Modell trainiert.
Stattdessen wird anhand des ausgegebenen Wahrscheinlichkeitsvektors das vom Modell prognostizierte nächste Zeichen
ausgewählt und der Sequenz angehängt.
Die neue Sequenz wird erneut in das Modell eingegegben und der Prozess beginnt erneut.
Dies wird so lange wiederholt, bis das Modell eine Stoppmarke («end») ausgibt (siehe Listing \ref{lst:generating-sequences}).

\begin{lstlisting}[language=Python, caption=Erzeugung von Sequenzen, label=lst:generating-sequences]
...

# Have model predict probabilities for next char
p = model.predict(x=np.array(padded_sequence)...)

# Randomly choose the next char with p as bias
last_char = np.random.choice(chars, p=p.reshape((96,)))

# Append next char to sequence
sequence += [last_char]

...
\end{lstlisting}

Der Aufruf \textit{model.predict(...)} gibt einen Wahrscheinlichkeitsvektor zurück, in dem jede Komponente der Wahrscheinlichkeit eines Zeichens im Vokabular entspricht,
nach der es aufgrund der bestehenden Sequenz folgen wird.
So wie in \autocite{dabbura} soll der nächste Buchstabe zufällig ausgewählt werden.
Dabei wählt die NumPy-Funktion \textit{random.choice()} das nächste Zeichen gemäss der Wahrscheinlichkeitsverteilung $ p $ aus.
Diese Methode garantiert, dass eine ins Modell eingegebene Initialsequenz jedes Mal andere Folgesequenzen erzeugt.
